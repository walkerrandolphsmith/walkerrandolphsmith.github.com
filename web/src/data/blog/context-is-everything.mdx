---
title: 'Context is Everything: How We Built a RAG System to Improve Branded, Culturally Aware Content'
date: 2025-06-05
tags: ['RAG', 'LLM', 'retrieval-augmented-generation', 'content-platform']
draft: false
excerpt: Large language models are impressive out of the boxâ€”but when your CMS powers content written by hundreds of people for a global audience, accuracy and voice consistency aren't optional. Pretrained knowledge wasn't enough. We needed context. Real context.
---

> **"GPT is good, but it doesnâ€™t know what we know."**  
> That was the insight that changed how we use LLMs at scale in our content platform.

Our content management system supports hundreds of writers creating content for a global audienceâ€”across dozens of languages, with strict brand voice requirements and awareness of cultural and geopolitical nuance.

We werenâ€™t just generating content. We were curating trust, clarity, and precision. And increasingly, our authors were turning to LLMs for help.

But we ran into a problem.

## ğŸ” The Problem: When Pretrained Isnâ€™t Enough

Out of the box, GPT and similar models are surprisingly capable. But theyâ€™re not trained on our internal editorial guides. They donâ€™t know which terms are preferred by our brand. And they certainly donâ€™t understand which regions require sensitivity around certain topicsâ€”because much of that information isnâ€™t even publicly available.

We saw hallucinations, tone mismatches, and subtle errors of implication that could be reputationally risky. LLMs were trying to be helpful, but without access to our internal context, they were guessing.

Worse: our CMS was filled with excellent source material. Past articles, internal references, editorial notes. But LLMs couldnâ€™t see any of it.

## ğŸ’¡ The Realization: Your CMS is a Knowledge Base

Rather than retrain a model from scratch (which was cost-prohibitive and brittle), we took a different approach: Retrieval-Augmented Generation (RAG).

If the model canâ€™t know everything up front, can we just teach it what it needs to knowâ€”right before it writes?

Turns out, yes. But only if you get retrieval right.

## ğŸ› ï¸ What We Built: A RAG Pipeline Backed by CMS Embeddings

We built a system that turned our CMS into a first-class retrieval layer:

- **Document Chunking**  
  Each article was chunked using a sliding window to preserve topic continuity across paragraphs, not just arbitrary token limits.

- **Embedding & Indexing**  
  Chunks were embedded and stored in a **hybrid vector search system**:

  - **Dense retrieval** via ANN (approximate nearest neighbor) for semantic similarity
  - **Sparse retrieval** via BM25 for keyword accuracy

- **Hybrid Search**  
  At query time, we blended results from both retrieval systems. This gave us both _contextually relevant_ and _lexically precise_ content references.

- **Prompt Construction**  
  Retrieved chunks were injected into a carefully structured prompt template that reflected our brand tone, editorial guidance, and task instruction.

- **Response Refinement**  
  We didnâ€™t just take the modelâ€™s first answer. We evaluated it against internal QA criteria and re-queried when needed. Eventually, this step was semi-automated using classification models trained on good/bad completions.

## ğŸ¤ Balancing Scale, Accuracy, and Sensitivity

One of the hardest challenges in scaling AI-generated content wasnâ€™t just technicalâ€”it was editorial.

Certain topics demand human judgment. Tone, regional nuance, and cultural sensitivity canâ€™t be fully automatedâ€”at least not without risk. So instead of trying to replace editors, we focused on **amplifying them**.

We introduced a **human-in-the-loop review system** that sat downstream of the RAG pipeline. Hereâ€™s how it worked:

- **Generated responses were scored** against editorial guidelines using lightweight heuristics and classifiers (e.g. tone, length, citation use).
- **Content that met a confidence threshold** could move directly to a quick human review with change suggestions.
- **Content that failed thresholds** was flagged for deeper editorial passâ€”with the original sources retrieved by the system for fast context.
- **Feedback from editors** was logged and used to tune retrieval weighting, prompt instructions, and re-query logic.

This gave us the best of both worlds:

- LLMs did the heavy lifting of draft generation and fact synthesis.
- Editors stayed focused on what only humans do well: voice, ethics, clarity, and emotional nuance.
- Costs dropped because **editors were reviewing, not writing from scratch**.
- Quality went up because **retrieved source material matched internal standards and previous work**.

Instead of slowing down creativity, the system created a **feedback loop that got better with time**. Every piece of content improved not just the article, but the pipeline itself.

## ğŸ§  What We Learned

RAG isnâ€™t just a technical pattern. Itâ€™s a lens into how your systems understand themselves.

- **Garbage in, garbage out** still appliesâ€”especially when "garbage" means "vague context."
- CMS metadata (like tags, authorship, and region) helped filter relevant chunks better than naive similarity.
- Editorial tone isnâ€™t just styleâ€”itâ€™s an instruction set. Embedding tone into prompts was critical.
- RAG became more than a writing aidâ€”it became a real-time way to distill institutional knowledge into every draft.

## âœ… Outcomes

The results were more than just faster content production:

- âœï¸ **Consistent tone** across teams and regions
- ğŸš« **Fewer hallucinations** and outdated facts
- ğŸ“ˆ **Improved editorial satisfaction**, especially among non-native English authors
- ğŸ” **Content reuse** improved, since past articles became more findable and referenceable via retrieval

RAG didnâ€™t replace writers. It made them faster, sharper, and better aligned with our collective voice.

## ğŸ“˜ Conclusion

Pretrained models are brilliant generalists. But when your platform operates at scale, across cultures and languages, you canâ€™t afford generalizations.

RAG gave us a way to amplify what we already knew. Not just factsâ€”but voice, context, and culture.

It reminded us that LLMs are only as smart as the systems that surround them.  
And that the best systems donâ€™t just generate contentâ€”they generate trust.
